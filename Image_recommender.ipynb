{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YxXeNZvYOeBj"
   },
   "outputs": [],
   "source": [
    "#!pip install tensorflow-gpu==2.0.0-beta1\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPool2D, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from google.colab import drive\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import pandas as pd\n",
    "from matplotlib.image import imread\n",
    "import glob\n",
    "from numpy import savez_compressed\n",
    "from sklearn.metrics import fbeta_score\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend\n",
    "import sys\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "import h5py\n",
    "from numpy import load\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from PIL import Image\n",
    "import imageio\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import os\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "faJnnXrPlGET"
   },
   "source": [
    "# Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U12ic0As1a_J"
   },
   "outputs": [],
   "source": [
    "### FUNCTIONS ###\n",
    "\n",
    "def create_labels_df(data_path):\n",
    "  ''' This funcitons grabs all files from a particular folder - extracts elements from the filenames and crates a df with filenames in one column and custom tags in another column '''\n",
    "  # Extract all filenames in folder, get the filenames of image dataset without file extensions\n",
    "  # Get the text before the '.' in a directory if it is indeed a file\n",
    "  image_labels = [f.split('.')[0] for f in listdir(data_path) if isfile(join(data_path, f))]\n",
    "\n",
    "  # Create tags for each image class (remove filename text after '__' to exclude augmentation and jpg)\n",
    "  image_tags = [f.split('__')[0] for f in listdir(data_path) if isfile(join(data_path, f))]\n",
    "\n",
    "  # Create a dataframe from image_labels and image_tags\n",
    "  labels_df = pd.DataFrame()\n",
    "  labels_df['Filenames'] = image_labels\n",
    "  labels_df['Tags'] = image_tags\n",
    "  print(labels_df.head())\n",
    "  return labels_df\n",
    "\n",
    "####################################\n",
    "\n",
    "def create_tag_mapping(labels_df):\n",
    "  ''' This function takes the tags from a column in a df - maps integers to each tag and vice verca '''\n",
    "  # Create labels for tags\n",
    "  labels = set()\n",
    "  for i in range(len(labels_df)):\n",
    "    # convert spaced separated tags into an array of tags\n",
    "    tags = labels_df['Tags'][i]#.split(' ') # split tag if necessary\n",
    "    # add tags to the set of known labels\n",
    "    labels.update(tags)\n",
    "  \n",
    "  # Convert labels set into a list\n",
    "  labels = list(labels)\n",
    "  # Sort alphabetically\n",
    "  labels.sort()\n",
    "\n",
    "  # Create dict that maps labels to integers and reverse\n",
    "  labels_map = {labels[i] : i for i in range(len(labels))}\n",
    "  inv_labels_map = {i : labels[i] for i in range(len(labels))}\n",
    "  return labels_map, inv_labels_map\n",
    "\n",
    "####################################\n",
    "\n",
    "def create_file_mapping(labels_df, filenames_col, tags_col):\n",
    "  mapping = dict()\n",
    "  for i in range(len(labels_df)):\n",
    "    name, tags = labels_df[filenames_col][i], labels_df[tags_col][i]\n",
    "    mapping[name] = tags#.split(' ')\n",
    "  return mapping\n",
    "\n",
    "####################################\n",
    "\n",
    "def one_hot_encode_single(tags, tag_to_int_map):\n",
    "  # create empty vector.\n",
    "\tencoding = np.zeros(len(tag_to_int_map), dtype='uint8')\n",
    "\t# mark 1 for each tag in the vector\n",
    "\tfor tag in tags:\n",
    "\t\tencoding[tag_to_int_map[tag]] = 1\n",
    "\treturn encoding\n",
    "\n",
    "####################################\n",
    "\n",
    "def oneHotEncode(path_to_files):\n",
    "  ''' Create one hot encodings for an ENTIRE LIST of tags'''\n",
    "  # get the filenames of image dataset without file extensions\n",
    "  # use set() to get only unique filenames\n",
    "  unique_image_labels = set([f.split('__')[0] for f in listdir(path_to_files) if isfile(join(path_to_files, f))])\n",
    "  list_image_labels = [f.split('__')[0] for f in listdir(path_to_files) if isfile(join(path_to_files, f))]\n",
    "\n",
    "  ### One-hot-encode label names in dataset ###\n",
    "  # The number of image categories - unique_image_labels\n",
    "  n_categories = len(unique_image_labels)\n",
    "\n",
    "  # The unique values of categories in the data\n",
    "  categories = np.array(unique_image_labels)\n",
    "\n",
    "  # Initialize ohe_labels as all zeros\n",
    "  ohe_labels = np.zeros((len(image_labels), n_categories))\n",
    "\n",
    "  # Loop over the labels\n",
    "  for ii in range(len(image_labels)):\n",
    "      # Find the location of this label in the categories variable\n",
    "      jj = np.where(categories == list_image_labels[ii])\n",
    "      # Set the corresponding zero to one\n",
    "      ohe_labels[ii, jj] = 1\n",
    "  return ohe_labels\n",
    "\n",
    "###################################\n",
    "\n",
    "def loadImages(path_to_images, image_filenames, IMG_HEIGHT, IMG_WIDTH):\n",
    "  '''Load image files from a directory into a variable as numpy arrays'''\n",
    "  # Create object to store images after importing and processing\n",
    "  loaded_images = []\n",
    "\n",
    "  #Load in and process images\n",
    "  for i in tqdm(range(len((image_filenames)))):\n",
    "    path = path_to_images + image_filenames[i] + '.jpg'\n",
    "    img = image.load_img(path, target_size=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "    img = image.img_to_array(img)\n",
    "    img = img/255.0\n",
    "    loaded_images.append(img)\n",
    "\n",
    "  loaded_images = np.array(loaded_images)\n",
    "  return loaded_images\n",
    "\n",
    "###################################\n",
    "\n",
    "def loadDataset_training(path_to_images, IMG_HEIGHT, IMG_WIDTH):\n",
    "  ''' Searach directory for files, extract filenames and tags into a df, Load each image, convert to ndarray, one hot encode filenames, save image into variable and corresponding encoded filename into another variable'''\n",
    "  # Extract all filenames and tags into a df\n",
    "  labels_df = create_labels_df(path_to_images)\n",
    "  \n",
    "  # Create objects to store each processed image and one hot encoded tag\n",
    "  processed_images = list()\n",
    "\n",
    "  for image_file in listdir(path_to_images):\n",
    "    # Define image path and filename\n",
    "    path = path_to_images + image_file\n",
    "    \n",
    "    # Load an image and convert to ndarray\n",
    "    loaded_image = image.img_to_array(image.load_img(path, target_size=(IMG_WIDTH, IMG_HEIGHT, 3)))\n",
    "\n",
    "    # Get corresponding tag from dataframe containing filenames and tags info\n",
    "    tags = labels_df['Tags']\n",
    "\n",
    "    # One hot encode corresponding tag\n",
    "    #target = oneHotEncode(path_to_images)\n",
    "\n",
    "    # Save image and ohe'd tag into separate variables as arrays\n",
    "    processed_images.append(loaded_image)\n",
    "  \n",
    "  # One hot encode the filenames\n",
    "  encoded_filenames = oneHotEncode(path_to_images)\n",
    "  # Save to \n",
    "  X = np.asarray(processed_images, dtype='uint8')\n",
    "  y = encoded_filenames\n",
    "  # save both arrays to one file in compressed format\n",
    "  savez_compressed('bike_parts.npz', X, y)\n",
    "\n",
    "  # Separate into train and test datasets\n",
    "  trainX, testX, trainY, testY = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "  print(trainX.shape, trainY.shape, testX.shape, testY.shape)\n",
    "  \n",
    "  return trainX, testX, trainY, testY\n",
    "\n",
    "#########################################################\n",
    "def loadDataset(path_to_images, IMG_HEIGHT, IMG_WIDTH):\n",
    "  ''' Searach directory for files, extract filenames and tags into a df, Load each image, convert to ndarray, one hot encode filenames, save image into variable and corresponding encoded filename into another variable'''\n",
    "  # Extract all filenames and tags into a df\n",
    "  labels_df = create_labels_df(path_to_images)\n",
    "  \n",
    "  # Create objects to store each processed image and one hot encoded tag\n",
    "  processed_images = list()\n",
    "\n",
    "  for image_file in listdir(path_to_images):\n",
    "    # Define image path and filename\n",
    "    path = path_to_images + image_file\n",
    "    \n",
    "    # Load an image and convert to ndarray\n",
    "    loaded_image = image.load_img(path, target_size=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "    loaded_image = image.img_to_array(loaded_image)\n",
    "    \n",
    "\n",
    "    # Get corresponding tag from dataframe containing filenames and tags info\n",
    "    tags = labels_df['Tags']\n",
    "\n",
    "    # One hot encode corresponding tag\n",
    "    #target = oneHotEncode(path_to_images)\n",
    "\n",
    "    # Save image and ohe'd tag into separate variables as arrays\n",
    "    processed_images.append(loaded_image)\n",
    "  \n",
    "  # One hot encode the filenames\n",
    "  encoded_filenames = oneHotEncode(path_to_images)\n",
    "  # Save to \n",
    "  X = np.asarray(processed_images, dtype='uint8')\n",
    "  y = encoded_filenames\n",
    "  # save both arrays to one file in compressed format\n",
    "  savez_compressed('bike_parts.npz', X, y)\n",
    "#########################################################\n",
    "\n",
    "def decode_predictions(path_to_files):\n",
    "  '''Function to map original filenames to ohe_labels, then decode predictions into original image filenames'''\n",
    "  # Get image labels from files in directory\n",
    "  image_labels = [f.split('.')[0] for f in listdir(path_to_files) if isfile(join(path_to_files, f))]\n",
    "  # Generate ohe_labels from filenames in directory\n",
    "  ohe_labels = oneHotEncode(training_data_path)\n",
    "  # Map image labels to ohe array in dictionary\n",
    "  mapping = dict()\n",
    "  for i in range(len(image_labels)):\n",
    "    label, ohe = image_labels[i], ohe_labels[i]\n",
    "    mapping[label] = ohe\n",
    "\n",
    "  # Get the indices of top 3 predictions\n",
    "  top3 = np.argsort(predictions[0])[:-4:-1] \n",
    "  \n",
    "  # Get ohe_labels based on indices in top3\n",
    "  top3_ohe_labels = []\n",
    "\n",
    "  for i in range(3):\n",
    "    top3_ohe_labels.append(ohe_labels[top3[i]])\n",
    "  # Turn ohe_labels list into an array\n",
    "  top3_array = np.array(top3_ohe_labels)\n",
    "  # Get image label corresponding to ohe_label in top3 predictions\n",
    "  pred_labels = []\n",
    "  for label, value in mapping.items():\n",
    "    for item in top3_array:\n",
    "      if np.array_equal(value, item):\n",
    "        pred_labels.append(label)\n",
    "  return pred_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CMZyy3IaOmUI",
    "outputId": "2416a507-b923-403d-9387-e16f419304d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Mount the google drive folder to colab notebook\n",
    "drive.mount('/content/gdrive')\n",
    "root_path = 'gdrive/My Drive/Image_Recommender/' # folder in gdrive\n",
    "\n",
    "# Define location datasets\n",
    "training_data_folder = 'gdrive/My Drive/Image_Recommender/Original/full_dataset/'\n",
    "augmented_data_path = 'gdrive/My Drive/Image_Recommender/Augmented_large/'\n",
    "test_folder_path = 'gdrive/My Drive/Image_Recommender/Test_images/'\n",
    "training_checkpoints_path = 'gdrive/My Drive/Image_Recommender/training_checkpoints/'\n",
    "augmented_full_dataset_path = 'gdrive/My Drive/Image_Recommender/Augmented_full_dataset/'\n",
    "augmented2 = 'gdrive/My Drive/Image_Recommender/Augmented2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9Fjhz0KNOcoM",
    "outputId": "229eeb9f-db0e-44fe-8c79-4abae150c805"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JATnpHrbpYA5"
   },
   "source": [
    "# Sort files in drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f76WVsnqpV0S"
   },
   "outputs": [],
   "source": [
    "def sort_files(parent_dir, daughter_dir):\n",
    "  \"\"\" \n",
    "  This function creates a new daughter directory within the parent directory, then sorts files with the same filenames inside the parent directory into filename-specific folders inside the daughter directory\n",
    "  \"\"\"\n",
    "  # Define the dir paths and create a new sub-dir if it does not already exist\n",
    "  if not os.path.exists(daughter_dir):\n",
    "    os.makedirs(daughter_dir)\n",
    "  \n",
    "  # Iterate through all files inside parent folder\n",
    "  for imagefile in os.listdir(parent_dir):\n",
    "    if os.path.isfile(parent_dir + imagefile) == True:\n",
    "      if fnmatch.fnmatch(imagefile, '*.jpg'):\n",
    "        \n",
    "        try:\n",
    "          # Grab filenames, paths \n",
    "          filename = imagefile.split('.jpg')[0]\n",
    "          if not os.path.exists(daughter_dir + filename):   \n",
    "            new_file_dir = daughter_dir + filename \n",
    "        except Exception as err:\n",
    "          print(\"Error when grabbing filenames {}\".format(err))   \n",
    "        \n",
    "        # Check if directory exists or not \n",
    "        if not os.path.exists(new_file_dir):\n",
    "          try:\n",
    "            os.makedirs((new_file_dir))\n",
    "          except Exception as err:\n",
    "            print('Error occurred during new folder creation err :{}'.format(err)) \n",
    "        \n",
    "        # If directory exists, move the file into its respective folder\n",
    "        if os.path.exists(new_file_dir):\n",
    "            try:\n",
    "              old_file_dir = parent_dir + imagefile \n",
    "              shutil.move(old_file_dir, new_file_dir)\n",
    "              print('Moved :{}'.format(imagefile))\n",
    "            except Exception as err:\n",
    "              print('Error occurred during moving :{}'.format(err))\n",
    "  \n",
    "  return print('All files sorted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wff21O6WPBmS",
    "outputId": "5a86958a-5692-44d9-87f1-ac861ab0d888"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files sorted\n"
     ]
    }
   ],
   "source": [
    "parent_dir = 'gdrive/My Drive/Image_Recommender/Original/full_dataset/'\n",
    "daughter_dir = 'gdrive/My Drive/Image_Recommender/' + 'Sorted_images/'\n",
    "sort_files(parent_dir, daughter_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LuKfUqqBE0Rh",
    "outputId": "de5486d9-de00-4ecc-efd6-b6e5e6384924"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8141\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir(daughter_dir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fu3Q7d2GeXp7"
   },
   "source": [
    "# Augment images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "gZGlXG0_eUtk",
    "outputId": "44e2389b-c020-4ea9-fc25-adac83853382"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zadelpen-hevelmoer-m8.png  save original image\n",
      "zadelpen-hevelmoer-m8.png  at motionblur\n",
      "zadelpen-hevelmoer-m8.png  at flipped\n",
      "zadelpen-hevelmoer-m8.png  at sharpen\n",
      "zadelpen-hevelmoer-m8.png  at flipud + sharpen\n",
      "zadelpen-hevelmoer-m8.png  at motionblur + sharpen\n",
      "sks-spanningmeter-airchecker-3-5-10-bar.png  save original image\n",
      "sks-spanningmeter-airchecker-3-5-10-bar.png  at motionblur\n",
      "sks-spanningmeter-airchecker-3-5-10-bar.png  at flipped\n",
      "sks-spanningmeter-airchecker-3-5-10-bar.png  at sharpen\n",
      "sks-spanningmeter-airchecker-3-5-10-bar.png  at flipud + sharpen\n",
      "sks-spanningmeter-airchecker-3-5-10-bar.png  at motionblur + sharpen\n",
      "spanninga-koplamp-trigon-10-luxe-vork-bevestiging.png  save original image\n",
      "spanninga-koplamp-trigon-10-luxe-vork-bevestiging.png  at motionblur\n",
      "spanninga-koplamp-trigon-10-luxe-vork-bevestiging.png  at flipped\n",
      "spanninga-koplamp-trigon-10-luxe-vork-bevestiging.png  at sharpen\n",
      "spanninga-koplamp-trigon-10-luxe-vork-bevestiging.png  at flipud + sharpen\n",
      "spanninga-koplamp-trigon-10-luxe-vork-bevestiging.png  at motionblur + sharpen\n",
      "spanninga-koplamp-trigon-15-luxe-zwart-usb.png  save original image\n",
      "spanninga-koplamp-trigon-15-luxe-zwart-usb.png  at motionblur\n",
      "spanninga-koplamp-trigon-15-luxe-zwart-usb.png  at flipped\n",
      "spanninga-koplamp-trigon-15-luxe-zwart-usb.png  at sharpen\n",
      "spanninga-koplamp-trigon-15-luxe-zwart-usb.png  at flipud + sharpen\n",
      "spanninga-koplamp-trigon-15-luxe-zwart-usb.png  at motionblur + sharpen\n",
      "spanninga-koplamp-trigon-25-luxe-zwart-usb.png  save original image\n",
      "spanninga-koplamp-trigon-25-luxe-zwart-usb.png  at motionblur\n",
      "spanninga-koplamp-trigon-25-luxe-zwart-usb.png  at flipped\n",
      "spanninga-koplamp-trigon-25-luxe-zwart-usb.png  at sharpen\n",
      "spanninga-koplamp-trigon-25-luxe-zwart-usb.png  at flipud + sharpen\n",
      "spanninga-koplamp-trigon-25-luxe-zwart-usb.png  at motionblur + sharpen\n",
      "spanninga-koplamp-trigon-25-lux-vork-bevestiging.png  save original image\n",
      "spanninga-koplamp-trigon-25-lux-vork-bevestiging.png  at motionblur\n",
      "spanninga-koplamp-trigon-25-lux-vork-bevestiging.png  at flipped\n",
      "spanninga-koplamp-trigon-25-lux-vork-bevestiging.png  at sharpen\n",
      "spanninga-koplamp-trigon-25-lux-vork-bevestiging.png  at flipud + sharpen\n",
      "spanninga-koplamp-trigon-25-lux-vork-bevestiging.png  at motionblur + sharpen\n",
      "spatbord-set-atb-24-zwart.png  save original image\n",
      "spatbord-set-atb-24-zwart.png  at motionblur\n",
      "spatbord-set-atb-24-zwart.png  at flipped\n",
      "spatbord-set-atb-24-zwart.png  at sharpen\n",
      "spatbord-set-atb-24-zwart.png  at flipud + sharpen\n",
      "spatbord-set-atb-24-zwart.png  at motionblur + sharpen\n",
      "spatbord-set-atb-26-zwart.png  save original image\n",
      "spatbord-set-atb-26-zwart.png  at motionblur\n",
      "spatbord-set-atb-26-zwart.png  at flipped\n",
      "spatbord-set-atb-26-zwart.png  at sharpen\n",
      "spatbord-set-atb-26-zwart.png  at flipud + sharpen\n",
      "spatbord-set-atb-26-zwart.png  at motionblur + sharpen\n",
      "tektro-schijfremset-achter-aquila-md-m500r-zwart.png  save original image\n",
      "tektro-schijfremset-achter-aquila-md-m500r-zwart.png  at motionblur\n",
      "tektro-schijfremset-achter-aquila-md-m500r-zwart.png  at flipped\n",
      "tektro-schijfremset-achter-aquila-md-m500r-zwart.png  at sharpen\n",
      "tektro-schijfremset-achter-aquila-md-m500r-zwart.png  at flipud + sharpen\n",
      "tektro-schijfremset-achter-aquila-md-m500r-zwart.png  at motionblur + sharpen\n",
      "topeak-drager-explorer-disc.png  save original image\n",
      "topeak-drager-explorer-disc.png  at motionblur\n",
      "topeak-drager-explorer-disc.png  at flipped\n",
      "topeak-drager-explorer-disc.png  at sharpen\n",
      "topeak-drager-explorer-disc.png  at flipud + sharpen\n",
      "topeak-drager-explorer-disc.png  at motionblur + sharpen\n",
      "topeak-drager-super-tourist-683.png  save original image\n",
      "topeak-drager-super-tourist-683.png  at motionblur\n",
      "topeak-drager-super-tourist-683.png  at flipped\n",
      "topeak-drager-super-tourist-683.png  at sharpen\n",
      "topeak-drager-super-tourist-683.png  at flipud + sharpen\n",
      "topeak-drager-super-tourist-683.png  at motionblur + sharpen\n",
      "topeak-drager-super-tourist-disc.png  save original image\n",
      "topeak-drager-super-tourist-disc.png  at motionblur\n",
      "topeak-drager-super-tourist-disc.png  at flipped\n",
      "topeak-drager-super-tourist-disc.png  at sharpen\n",
      "topeak-drager-super-tourist-disc.png  at flipud + sharpen\n",
      "topeak-drager-super-tourist-disc.png  at motionblur + sharpen\n",
      "topeak-drager-super-tourist-dx-684.png  save original image\n",
      "topeak-drager-super-tourist-dx-684.png  at motionblur\n",
      "topeak-drager-super-tourist-dx-684.png  at flipped\n",
      "topeak-drager-super-tourist-dx-684.png  at sharpen\n",
      "topeak-drager-super-tourist-dx-684.png  at flipud + sharpen\n",
      "topeak-drager-super-tourist-dx-684.png  at motionblur + sharpen\n",
      "topeak-drager-super-tourist-dx-disc.png  save original image\n",
      "topeak-drager-super-tourist-dx-disc.png  at motionblur\n",
      "topeak-drager-super-tourist-dx-disc.png  at flipped\n",
      "topeak-drager-super-tourist-dx-disc.png  at sharpen\n",
      "topeak-drager-super-tourist-dx-disc.png  at flipud + sharpen\n",
      "topeak-drager-super-tourist-dx-disc.png  at motionblur + sharpen\n",
      "topeak-drager-super-tourist-fat-dis.png  save original image\n",
      "topeak-drager-super-tourist-fat-dis.png  at motionblur\n",
      "topeak-drager-super-tourist-fat-dis.png  at flipped\n",
      "topeak-drager-super-tourist-fat-dis.png  at sharpen\n",
      "topeak-drager-super-tourist-fat-dis.png  at flipud + sharpen\n",
      "topeak-drager-super-tourist-fat-dis.png  at motionblur + sharpen\n",
      "topeak-koplamp-whitelite-mini-usb-wit.png  save original image\n",
      "topeak-koplamp-whitelite-mini-usb-wit.png  at motionblur\n",
      "topeak-koplamp-whitelite-mini-usb-wit.png  at flipped\n",
      "topeak-koplamp-whitelite-mini-usb-wit.png  at sharpen\n",
      "topeak-koplamp-whitelite-mini-usb-wit.png  at flipud + sharpen\n",
      "topeak-koplamp-whitelite-mini-usb-wit.png  at motionblur + sharpen\n",
      "topeak-koplamp-whitelite-mini-usb-zwart.png  save original image\n",
      "topeak-koplamp-whitelite-mini-usb-zwart.png  at motionblur\n",
      "topeak-koplamp-whitelite-mini-usb-zwart.png  at flipped\n",
      "topeak-koplamp-whitelite-mini-usb-zwart.png  at sharpen\n",
      "topeak-koplamp-whitelite-mini-usb-zwart.png  at flipud + sharpen\n",
      "topeak-koplamp-whitelite-mini-usb-zwart.png  at motionblur + sharpen\n",
      "schwalbe-buitenband-28x1-25-32-622-roadcruiser-k-guard-zwart.png  save original image\n",
      "schwalbe-buitenband-28x1-25-32-622-roadcruiser-k-guard-zwart.png  at motionblur\n",
      "schwalbe-buitenband-28x1-25-32-622-roadcruiser-k-guard-zwart.png  at flipped\n",
      "schwalbe-buitenband-28x1-25-32-622-roadcruiser-k-guard-zwart.png  at sharpen\n",
      "schwalbe-buitenband-28x1-25-32-622-roadcruiser-k-guard-zwart.png  at flipud + sharpen\n",
      "schwalbe-buitenband-28x1-25-32-622-roadcruiser-k-guard-zwart.png  at motionblur + sharpen\n",
      "michelin-buitenband-275-x-280-71-584-e-wild-achter-gum-x-vouw-zwart.png  save original image\n",
      "michelin-buitenband-275-x-280-71-584-e-wild-achter-gum-x-vouw-zwart.png  at motionblur\n",
      "michelin-buitenband-275-x-280-71-584-e-wild-achter-gum-x-vouw-zwart.png  at flipped\n",
      "michelin-buitenband-275-x-280-71-584-e-wild-achter-gum-x-vouw-zwart.png  at sharpen\n",
      "michelin-buitenband-275-x-280-71-584-e-wild-achter-gum-x-vouw-zwart.png  at flipud + sharpen\n",
      "michelin-buitenband-275-x-280-71-584-e-wild-achter-gum-x-vouw-zwart.png  at motionblur + sharpen\n",
      "polisport-spatbordset-towny-28-46mm-zilver.jpg  at motionblur\n",
      "polisport-spatbordset-towny-28-46mm-zilver.jpg  at sharpen\n",
      "polisport-spatbordset-towny-28-46mm-zilver.jpg  at motionblur + sharpen\n",
      "polisport-spatbordset-towny-28-51mm-zilver.jpg  at motionblur\n",
      "polisport-spatbordset-towny-28-51mm-zilver.jpg  at motionblur + sharpen\n",
      "polisport-spatbordset-towny-28-51mm-zwart.jpg  at motionblur\n"
     ]
    }
   ],
   "source": [
    "def augment_images(training_data_path, augmented_data_path):\n",
    "  #### WORKING CODE FOR IMAGE AUGMENTATIONS ####\n",
    "  \n",
    "  for imagefile in listdir(training_data_path):\n",
    "    \n",
    "    try:\n",
    "      # Get the image filename\n",
    "      image_label = imagefile.split('.')[0]\n",
    "    except:\n",
    "      print(imagefile, ' at extracting image label')\n",
    "\n",
    "    try:\n",
    "      # Load an image with imageio (expand to many images later)\n",
    "      loaded_image = imageio.imread(training_data_path + imagefile)\n",
    "    except:\n",
    "      print(imagefile, ' at loading image')\n",
    "    \n",
    "    try:\n",
    "      # Save the original image in the augmented folder\n",
    "      im = Image.fromarray(loaded_image)\n",
    "      im.save(augmented_data_path + image_label, 'JPEG')\n",
    "    except:\n",
    "      print(imagefile, ' save original image')\n",
    "\n",
    "    try:\n",
    "      # Apply motionblur and save into a new object then save to disk\n",
    "      img_motionblur = iaa.MotionBlur(k=15, angle=45)(images=loaded_image)\n",
    "      im = Image.fromarray(img_motionblur)\n",
    "      im.save(augmented_data_path + image_label + '__motionblur', 'JPEG')\n",
    "    except:\n",
    "      print(imagefile, ' at motionblur')\n",
    "\n",
    "    try:\n",
    "      # Apply flip and save into a new object then save to disk\n",
    "      img_flipped = iaa.Flipud(1.0)(images=loaded_image)\n",
    "      im = Image.fromarray(img_flipped)\n",
    "      im.save(augmented_data_path + image_label + '__flipped', 'JPEG')\n",
    "    except:\n",
    "      print(imagefile, ' at flipped')\n",
    "\n",
    "    try:\n",
    "      # Apply sharpen (detect outlines) and save into a new object then save to disk\n",
    "      img_sharpen = iaa.Sharpen(alpha=1, lightness=0)(images=loaded_image)\n",
    "      im = Image.fromarray(img_sharpen)\n",
    "      im.save(augmented_data_path + image_label + '__sharpen', 'JPEG')\n",
    "    except:\n",
    "      print(imagefile, ' at sharpen')\n",
    "\n",
    "    try:\n",
    "      # Apply a flipud (horizontal) + sharpen to each image then save to disk\n",
    "      img_flip_sharpen = iaa.Sequential([\n",
    "          iaa.Flipud(1.0),\n",
    "          iaa.Sharpen(alpha=1, lightness=0)])(images=loaded_image)\n",
    "      im = Image.fromarray(img_flip_sharpen)\n",
    "      im.save(augmented_data_path + image_label + '__img_flip_sharpen', 'JPEG')\n",
    "    except:\n",
    "      print(imagefile, ' at flipud + sharpen')\n",
    "\n",
    "    try:\n",
    "      # Apply a motionblur then sharpen to each image then save to disk\n",
    "      img_motionblur_sharpen = iaa.Sequential([\n",
    "                                  iaa.MotionBlur(k=15, angle=45),\n",
    "                                  iaa.Sharpen(alpha=1, lightness=0)])(images=loaded_image)\n",
    "      im = Image.fromarray(img_motionblur_sharpen)\n",
    "      im.save(augmented_data_path + image_label + '__motionblur_sharpen', 'JPEG')\n",
    "    except:\n",
    "      print(imagefile, ' at motionblur + sharpen')\n",
    "\n",
    "# RUN IT!\n",
    "#augment_images(training_data_folder, augmented2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "colab_type": "code",
    "id": "WMpFID8IUtXM",
    "outputId": "771ffbf4-a8b2-481f-8232-5c12971e03ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Filenames                                             Tags\n",
      "0  bafang-schijfring-met-nok-voor-front-hub-motor...   bafang-schijfring-met-nok-voor-front-hub-motor\n",
      "1  bafang-schijfring-met-nok-voor-front-hub-motor...   bafang-schijfring-met-nok-voor-front-hub-motor\n",
      "2  bafang-schijfring-met-nok-voor-front-hub-motor...   bafang-schijfring-met-nok-voor-front-hub-motor\n",
      "3  bafang-lader-acculader-43-volt-2a-chg-c01-2a-e...  bafang-lader-acculader-43-volt-2a-chg-c01-2a-en\n",
      "4  bafang-lader-acculader-43-volt-2a-chg-c01-2a-e...  bafang-lader-acculader-43-volt-2a-chg-c01-2a-en\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-18be4a6c05fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# load the jpeg images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugmented_data_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_mapping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;31m# save both arrays to one file in compressed format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-18be4a6c05fb>\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, file_mapping, tag_mapping)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mphoto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphoto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'uint8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0;31m# get tags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0;31m# one hot encode tags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sapim-spaak-14-242-rvs-zonder-nippel__motionblur'"
     ]
    }
   ],
   "source": [
    "##### LATEST PIPELINE #####\n",
    "\n",
    "def create_labels_df(data_path):\n",
    "  ''' This funcitons grabs all files from a particular folder - extracts elements from the filenames and crates a df with filenames in one column and custom tags in another column '''\n",
    "  # Extract all filenames in folder, get the filenames of image dataset without file extensions\n",
    "  # Get the text before the '.' in a directory if it is indeed a file\n",
    "  image_labels = [f.split('.')[0] for f in listdir(data_path) if isfile(join(data_path, f))]\n",
    "\n",
    "  # Create tags for each image class (remove filename text after '__' to exclude augmentation and jpg)\n",
    "  image_tags = [f.split('__')[0] for f in listdir(data_path) if isfile(join(data_path, f))]\n",
    "\n",
    "  # Create a dataframe from image_labels and image_tags\n",
    "  labels_df = pd.DataFrame()\n",
    "  labels_df['Filenames'] = image_labels\n",
    "  labels_df['Tags'] = image_tags\n",
    "  print(labels_df.head())\n",
    "  return labels_df\n",
    "\n",
    "def create_tag_mapping(labels_df):\n",
    "  ''' This function takes the tags from a column in a df - maps integers to each tag and vice verca '''\n",
    "  # Create labels for tags\n",
    "  labels = set()\n",
    "  for i in range(len(labels_df)):\n",
    "    # convert spaced separated tags into an array of tags\n",
    "    tags = labels_df['Tags'][i].split(' ') # use split to ensure the whole tag, not jsut single characters, are plaved into set() object\n",
    "    # add tags to the set of known labels\n",
    "    labels.update(tags)\n",
    "  \n",
    "  # Convert labels set into a list\n",
    "  labels = list(labels)\n",
    "  # Sort alphabetically\n",
    "  labels.sort()\n",
    "\n",
    "  # Create dict that maps labels to integers and reverse\n",
    "  labels_map = {labels[i] : i for i in range(len(labels))}\n",
    "  inv_labels_map = {i : labels[i] for i in range(len(labels))}\n",
    "  return labels_map, inv_labels_map\n",
    "\n",
    "####################################\n",
    "\n",
    "def create_file_mapping(labels_df, filenames_col, tags_col):\n",
    "  mapping = dict()\n",
    "  for i in range(len(labels_df)):\n",
    "    name, tags = labels_df['Filenames'][i], labels_df['Tags'][i]\n",
    "    mapping[name] = tags#.split(' ')\n",
    "  return mapping\n",
    "\n",
    "# create a one hot encoding for one list of tags\n",
    "def one_hot_encode(tags, mapping):\n",
    "\t# create empty vector\n",
    "\tencoding = np.zeros(len(mapping), dtype='uint8')\n",
    "\t# mark 1 for each tag in the vector\n",
    "\tfor tag in tags:\n",
    "\t\tencoding[mapping[tags]] = 1\n",
    "\treturn encoding\n",
    "\n",
    "# load all images into memory\n",
    "def load_dataset(path, file_mapping, tag_mapping):\n",
    "\tphotos, targets = list(), list()\n",
    "\t# enumerate files in the directory\n",
    "\tfor filename in listdir(folder):\n",
    "\t\t# load image\n",
    "\t\tphoto = load_img(path + filename, target_size=(128,128))\n",
    "\t\t# convert to numpy array\n",
    "\t\tphoto = img_to_array(photo, dtype='uint8')\n",
    "\t\t# get tags\n",
    "\t\ttags = file_mapping[filename]\n",
    "\t\t# one hot encode tags\n",
    "\t\ttarget = one_hot_encode(tags, tag_mapping)\n",
    "\t\t# store\n",
    "\t\tphotos.append(photo)\n",
    "\t\ttargets.append(target)\n",
    "\tX = np.array(photos, dtype='uint8')\n",
    "\ty = np.array(targets, dtype='uint8')\n",
    "\treturn X, y\n",
    "\n",
    "labels_df = create_labels_df(augmented_full_dataset_path)\n",
    "# create a mapping of tags to integers\n",
    "tag_mapping, _ = create_tag_mapping(labels_df)\n",
    "# create a mapping of filenames to tag lists\n",
    "file_mapping = create_file_mapping(labels_df, 'Filenames', 'Tags')\n",
    "# load the jpeg images\n",
    "folder = augmented_data_path\n",
    "X, y = load_dataset(folder, file_mapping, tag_mapping)\n",
    "print(X.shape, y.shape)\n",
    "# save both arrays to one file in compressed format\n",
    "savez_compressed('bike_parts.npz', X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zEUnCk8OerOO"
   },
   "source": [
    "# Model using CGG from ImageNet with centered image pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5M0DN41jj_4x"
   },
   "source": [
    "# Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "omigfoKcj-ka",
    "outputId": "e38d9824-df92-47f7-aab7-afacc1cb1587"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-083c6e23833d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;31m# entry point, run the test harness\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m \u001b[0mrun_test_harness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-083c6e23833d>\u001b[0m in \u001b[0;36mrun_test_harness\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m#loadDataset(training_data_path, 128, 128)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# load dataset from saved npz file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;31m# create data generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mdatagen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturewise_center\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-083c6e23833d>\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m         \u001b[0;31m# load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bike_parts.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'arr_0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'arr_1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'bike_parts.npz'"
     ]
    }
   ],
   "source": [
    "# load train and test dataset\n",
    "def load_dataset():\n",
    "\t# load dataset\n",
    "\tdata = load('bike_parts.npz')\n",
    "\tX, y = data['arr_0'], data['arr_1']\n",
    "\treturn X, y\n",
    " \n",
    "# # calculate fbeta score for multi-class/label classification\n",
    "# def fbeta(y_true, y_pred, beta=2):\n",
    "# \t# clip predictions\n",
    "# \ty_pred = backend.clip(y_pred, 0, 1)\n",
    "# \t# calculate elements\n",
    "# \ttp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)), axis=1)\n",
    "# \tfp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0, 1)), axis=1)\n",
    "# \tfn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0, 1)), axis=1)\n",
    "# \t# calculate precision\n",
    "# \tp = tp / (tp + fp + backend.epsilon())\n",
    "# \t# calculate recall\n",
    "# \tr = tp / (tp + fn + backend.epsilon())\n",
    "# \t# calculate fbeta, averaged across each class\n",
    "# \tbb = beta ** 2\n",
    "# \tfbeta_score = backend.mean((1 + bb) * (p * r) / (bb * p + r + backend.epsilon()))\n",
    "# \treturn fbeta_score\n",
    " \n",
    "# define cnn model\n",
    "def define_model(in_shape=(128, 128, 3), out_shape=388):\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=in_shape)\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# allow last vgg block to be trainable\n",
    "\t# model.get_layer('block5_conv1').trainable = True\n",
    "\t# model.get_layer('block5_conv2').trainable = True\n",
    "\t# model.get_layer('block5_conv3').trainable = True\n",
    "\t# model.get_layer('block5_pool').trainable = True\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(out_shape, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = 'adam' #SGD(lr=0.01, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy')\n",
    "\treturn model\n",
    " \n",
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(history):\n",
    "\t# plot loss\n",
    "\tplt.subplot(211)\n",
    "\tplt.title('Cross Entropy Loss')\n",
    "\tplt.plot(history.history['loss'], color='blue', label='train')\n",
    "\tplt.plot(history.history['val_loss'], color='orange', label='test')\n",
    "\tplt.show()\n",
    "  # plot accuracy\n",
    "\tplt.subplot(212)\n",
    "\tplt.title('Fbeta')\n",
    "\tplt.plot(history.history['fbeta'], color='blue', label='train')\n",
    "\tplt.plot(history.history['val_fbeta'], color='orange', label='test')\n",
    "\tplt.show()\n",
    "  # save plot to file\n",
    "\tfilename = sys.argv[0].split('/')[-1]\n",
    "\tplt.savefig(filename + '_plot.png')\n",
    "\tplt.close()\n",
    " \n",
    "def run_test_harness():\n",
    "\t\n",
    "\t# load and process images and filenames then save into .npz file\n",
    "\t#loadDataset(training_data_path, 128, 128)\n",
    "\t# load dataset from saved npz file\n",
    "\tX, y = load_dataset()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# prepare iterator\n",
    "\ttrain_it = datagen.flow(X, y, batch_size=128)\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t\n",
    "\t# Save training progress\n",
    "\tcheckpoint_path = training_checkpoints_path\n",
    "\tcheckpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "\t# Create a callback that saves the model's weights\n",
    "\tcp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tsave_weights_only=True,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tverbose=1)\n",
    "\t# fit model\n",
    "\tmodel.fit_generator(train_it, steps_per_epoch=len(train_it), epochs=50, verbose=1, callbacks=[cp_callback])\n",
    "\t# save model\n",
    "\tmodel.save('final_model.h5')\n",
    "  \n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0xxShTk6eTWa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K9El_A9IeW5g"
   },
   "source": [
    "# Predict on new image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "eI6QQT2FmSkJ",
    "outputId": "25931c7c-b713-4292-de78-70208caaabde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of top 3 predictions  [286  39 337]\n",
      "Predicted Tags  ['spanninga-koplamp-kendo-led-aan-uit-15-lux', 'bofix-flensmoer-m6-100-stuks', 'union-derailleur-pad-gh-225']\n"
     ]
    }
   ],
   "source": [
    "\t# convert a prediction to tags\n",
    "def top3_predictions(inv_mapping, predictions):\n",
    "\t# Get indices of top 3 results from predictions array\n",
    "\ttop3 = np.argsort(predictions)[:-4:-1] \n",
    "\tprint('Indices of top 3 predictions ', top3)\n",
    "\t# collect all predicted tags based on indices extracted in top3\n",
    "\ttags = [inv_mapping[i] for i in top3]\n",
    "\t#print('Predicted Tags ', tags)\t\n",
    "\treturn tags\n",
    " \n",
    "# load and prepare the image\n",
    "def load_image(filename):\n",
    "\t# load the image\n",
    "\timg = load_img(filename, target_size=(128, 128))\n",
    "\t# convert to array\n",
    "\timg = img_to_array(img)\n",
    "\t# reshape into a single sample with 3 channels\n",
    "\timg = img.reshape(1, 128, 128, 3)\n",
    "\t# center pixel data\n",
    "\timg = img.astype('float32')\n",
    "\timg = img - [123.68, 116.779, 103.939]\n",
    "\treturn img\n",
    " \n",
    "# load an image and predict the class\n",
    "def run_example(inv_mapping, chosen_image):\n",
    "\t# load the image\n",
    "\timg = load_image(chosen_image)\n",
    "\t# load model\n",
    "\tmodel = load_model('final_model.h5')\n",
    "\t# predict the class\n",
    "\tresult = model.predict(img)\n",
    "\t#print(result[0])\n",
    "\t# map prediction to tags\n",
    "\ttags = top3_predictions(inv_mapping, result[0])\n",
    "\tprint('Predicted Tags ', tags)\n",
    " \n",
    "# Set filepath for image to predict\n",
    "chosen_image = test_folder_path + '3379399395040_01.jpg'\n",
    "\n",
    "# create a mapping of tags to integers from labels_df containing 'Filenames' and 'Tags'\n",
    "_, inv_mapping = create_tag_mapping(labels_df)\n",
    "# entry point, run the example\n",
    "run_example(inv_mapping, chosen_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "szJqJMgCxfTA"
   },
   "source": [
    "# Convert top 3 predictions to filename or tag ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D0TJIX91lbHw"
   },
   "source": [
    "# Image augmentation pipeline setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "93PTEtLwN3YF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OL8cxsbAQWcS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m0BrxdvXhjxQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Image_recommender_3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
